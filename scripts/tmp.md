(1)国内外研究动态语音情感识别(SpeechEmotionRecognition，SER)在人机交互过程中发挥极为重要的作用，近年来该领域的研究越来越受到重视。同时也是目前人机交互等领域的热门研究话题。SER的主要目的是对语音信号按照不同的情感进行分类，比如“生气”、“恐惧”、“厌恶”、“高兴”等。由于语音情感识别在人机交互和情感计算中的重要性，使得其具有十分广泛的应用。在过去的几年里，该领域研究者已经提出了许多有效的方法来解决SER中出现的问题，这些方法大部分是集中在一个单一的语音数据库上进行的。大多数的语音情感识别方法主要在单一语音数据库上进行训练和测试。这些训练数据和测试数据具有相同的声学条件，在这种条件下，现有的语音情感识别技术已经实现了较高的性能。然而，在实际应用中训练和测试集可能来自不同的语音情感数据库。用于训练的语料库与测试语料库之间往往存在非常大的差异，现实世界中的条件会更加复杂，比如含有噪音或回音的声学条件。此外，训练数据库和测试数据库可能来自多种不同的语言、不同的说话人、不同的文化、不同的数据规模等差异。由于这种不同情感数据库间存在的巨大差异性，导致大多数现有的语音情感识别方法取得的跨库识别性能欠佳。无法满足实际应用的需要。因此，跨数据库(Cross-corpus)的语音情感识别越来越受到重视。
情感特征提取方法的进展:在手工特征提取方面，基础的方法是利用语料库归一化方法，将不同的语料库按照同样的方法进行归一化从而降低两个语料库之间的差异。这种方法虽然在一定程度上缓解了语料库之间的差异，但不同语料库之间的特征分布依然大不相同。之后出现了不同的测量语料库数据分布差异的方法，如MMD准则。许多研究者使用不同的方法将源语料库和目标语料库的样本映射到同一特征空间并在这个特征空间中利用MMD准则测量两个数据库之间的特征分布差异，最后最小化这个差异以得到域不变特征。受MMD准则的启发，一些方法联合考虑边际概率分布和条件分布，使边际分布之间的距离以及条件分布之间的距离最小，从而消除两个数据库特征分布之间的差异。除此之外，一些方法引入正则化约束来缓解两个语料库的分布差异或者利用梯度反转层形成对抗训练得到域不变特征。
为了弥补手工域特征提取方法的不足，研究者们利用深度学习技术进行自动化提取高层次特征，将提取到的深度特征进行类似于特征变换的归一化以缓解不同数据库之间的差异。一些研究者将提取到的深度低维特征运用MMD准则测量不同数据库之间的特征分布差异并最小化这种差异。使用自编码器对输入特征进行编码-解码的方法开始被应用于跨库语音情感识别，来最小化重构损失以减少域间差异。随着对抗学习的出现，一些研究者开始将对抗训练运用于跨库语音情感识别域不变特征，在此基础上有些方法将对抗训练与MMD准则相结合以减少数据库之间的特征分布差异[1]。
跨库语音情感识别是SER领域的新兴研究方向，相关的具体研究包括：Schuller等[2]从唤醒程度和效价维度两个方面进行了跨库语音情感识别的研究；Jeon等[3]初步探讨了跨库SER的相关内容；Deng等[4]采用无监督学习的自适应自动编码器处理跨库语音情感识别的问题；Song等[5]基于对混合因子的分析，进而研究讨论了跨库条件下的语音情感识别；张昕然[6]提出带有无限成分数的t分布混合模型（iSMM）来提高识别跨库SER性能。
Chang等[7]提出了一种多任务深度卷积生成对抗网络,用于从未标记数据上的计算谱图中学习强特征表示,并以情绪效价为主要目标,情绪激活为次要目标进行多任务学习。Deng等[8]提出了一种半监督自编码器来提高跨库语音情感识别性能。在监督学习中添加一个除情感类外额外的类,当监督分类器从给定的标记数据中学习时将所有未标记的数据预测为这个额外的类。使模型可以从标记数据和未标记数据的组合中获益。该方法将无监督自编码器与深度前馈网络的监督学习目标连接,构造联合优化目标函数,确保在标记和未标记数据上最小化无监督目标的重建误差以及由监督目标测量的预测误差。Latif等[9]提出了一种基于DBNs的迁移学习技术,使用的DBN由3个RBM层组成,其中前2个RBM包含1000个隐藏神经元,第3个RBM包含2000个隐藏神经元。采用包括韵律特征和谱特征等88个特征的eGeMAPS特征集作为DBNs的输入。结果表明,与稀疏自编码器和SVM相比,DBNs在跨库语音情感识别上表现出更好的性能。Gideon等[10]提出了一个对抗鉴别域泛化算法,该算法在特征编码和情感分类的基础上增加了测量不同数据集之间距离的模块,并迭代的将每个数据集学习到的表示移动的更近,采用生成对抗网络的思想,该算法可以充分利用未标记的测试数据在不同数据集上获得域不变特征表示。他们提取40维Mel滤波器组作为算法输入,得到的实验结果表明,该算法表现优于CNNs。Latif等[11]提出了一种多任务半监督对抗自编码(adversarialautoencoding,AAE)方法,利用短时傅里叶变换(shorttimeFouriertransform,STFT)得到的谱图作为AAE的输入,在对抗性自编码器中生成潜在表示,之后构建一个使用大量可用数据的多任务学习框架,将情感、说话人和性别识别作为辅助任务,利用一些有限的可用数据最终可以获得比CNN、CNN+LSTM和DBN更好的性能。Parthasarathy等[12]提出了一种结合无监督辅助任务的阶梯网络半监督方法,其中首要任务是预测维度情感属性,辅助任务用去噪自编码器产生中间特征表示的重构并以半监督的方式对来自目标域的大量未标记数据进行训练。以INTERSPEECH2013特征集作为阶梯网络的输入,研究表明,与完全监督单任务学习(single-tasklearning,STL)和多任务学习基线相比,所提方法取得了优越的性能。刘雨柔[13]基于同库实验，将VMD算法应用于提取GFCC特征的过程中，提出一种情感语音特征（谱特征）VGFCC，以及一种复合网络SSAE-KELM，提高了情感识别率。
从效果上看，目前已知有多种可以提升跨库语音识别的跨库技术，其中最主要的方法是利用自编码器和基于对抗学习等域自适应方法得到域不变特征来提高分类器对目标域数据集的识别精度。
（2）课题意义
利用计算机从语音信号中自动识别出说话人的情感状态，是实现自然人机交互界面的关键前提。借助语音情感识别系统，可以提升用户操作计算机的体验。由于跨库语音情感识别涉及多个学科，因此对该领域的研究还有助于促进相关学科的共同进步，具有重要的使用价值和理论价值。
语音情感识别相关技术还是情感计算的重要内容，对其理论和应用的研究必然有助于推动情感计算的发展。跨库语音情感识别系统考虑的条件相比于单数据库训练出来的SER系统更加接近实际生活，具有更好的鲁棒性和实用性，更加适应实际生活的识别要求，跨库技术的研究使SER系统在使用的时候更加有效。二、研究的基本内容，拟解决的主要问题：本课题针对跨数据库语音情感识别技术进行相关研究，基于现有的基本理论和方法以设计和实现一个具有较好地进行跨库识别的SER系统。基于已有的多个情感语料库，提取有效的跨语料库情感特征，建立分类模型，然后对新的语音进行情感分类，最后完成语音情感识别系统的开发。（1）情感语料库的选择跨数据库SER训练集与测试集的语音要求来自不同数据库，研究中需要采用多种语料库。跨库分为两类，一类是跨同一语言不同库，另一类是跨不同语言不同库。不同的语料库在录制时，说话人不同，其语言、文化背景各色迥异，语音中所含情感会产生巨大差异，而且录制的设备、标准、环境大不相同，都会导致声学信息严重失配，影响结果。为了能够较好的模拟现实中的复杂语音，语料库中的语音数据应该具有足够的复杂度。因为现实环境中的语音环境和录制条件往往比专门的录音环境更加复杂，如果选用的语料库应该包含非情感因素，有利于检验跨库SER的鲁棒性。不同语料库所包含的情感应该相对一致，便于跨库识别任务地有效进行。
部分数据集不仅记录了语音情感标签,而且保留了说话人和性别相关信息。这便于研究者利用这些信息进行与说话人独立、与性别独立的语音情感识别以及说话人识别、性别识别或将两者作为语音情感识别的辅助任务进行多任务学习。
除了说话人间的个体差异，人类表达情感的方式不仅是语音,还有面部表情、文本和肢体动作等，如果综合考虑语音之外的其他情感表达式对语音情感的影响，可能有助于提高跨库SER的性能，语料库除了只包含语音的单模态数据集之外,最好还有包含语音、文本或视频的多模态数据集,如IEMOCAP、MSP-Improv，SAVEE等。（2）语音特征的提取目前能充分有效描述情感信息的特征比较缺乏，特别是能够用于跨库语音情感识别的域不变特征更为稀缺，这也是当前跨库SER的性能普遍不高的主要原因；人类交流的语音中包含语言的种类、蕴意以及语气中的情感等信息，理想的情感语音特征应独立于说话人、语种、语义等客观因素，尽可能全面有效地体现情感信息，跨库SER对情感语音特征的泛化性要求更高，已有的研究从各方面分析语音特性提取特征，普遍使用的有韵律、音质、基于谱以及非线性特征，针对特征级融合的研究也小有成效，但获取最优情感语音特征仍是研究的重点。
传统的手工语音情感特征多为简单的浅层特征,只能包含语音信号的部分信息，且费时费力。想要进一步提高SER在跨库识别任务中的性能，需要借助深度学习技术学习具有鲁棒性的深层特征。
另一方面，不同的深度学习方法之间的组合可能得到更有利于提高性能的深层特征，合理地组合搭配深度学习技术也是重要问题之一。
（3）分类模型的构建：
通过优化分类器的性能来提高情感的识别率是重要的研究方向。近年来在情感计算领域，包括重回归分析和主元素分析等多变量解析方法神经网络技术，都取得了一定的进展，但由于情感特征学习收敛性问题，还存在一定的局限性。另外，采用主元分析法、最大似然贝叶斯分类器和K最近邻分类器、人工神经元网络、隐马尔科夫模型等技术的情感计算方法，也取得了一定的成果。这些传统的分类方法有各自的局限性，本系统拟采用经过优化的分类模型。
（4）语音情感识别系统的开发：
本系统包含情感语料库数据预处理，情感特征提取和优选，情感分类器的改进。
系统能够可视化地展示模型的训练过程和语音情感的识别过程。允许用户选择语音文件送入系统进行分析和识别，并给出识别结果和过程性信息。三、研究步骤、方法及措施：步骤和流程
1.查找语音情感识别相关文献并阅读，了解基本的语音情感识别算法，分析基础算法可能导致算法在跨库识别效果不佳的问题。
2.在网上收集实验数据，同时对查找的数据进行筛选，选出常能够用于实现跨库语音识别系统设计的语音数据。
3.通过代码对论文中的语音情感识别算法进行实现，并通过实验对比和分析现有算法的识别效果和识别性能，确定需要解决的主要问题和具体问题。
4.根据对已有算法的分析，对实验进行总结，设计本文的跨库SER算法。将本文算法分成以下两个模块:跨库语音情感特征提取(域不变特征)提取、跨库情感分类器的设计和改进。
5.根据对跨库语音识别相关文献的阅读和分析发现，目前已有的算法主要针对单语料库具有较好的性能和准确率，在跨库识别上性能不足，对跨库语音情感特征的提取和处理是优化跨库识别的关键，自编码器的应用有助于提高识别性能。
6.使用足量的数据对优化后的跨库SER算法进行测试，根据测试效果对算法参数等细节再调整，进一步提升识别效果。完成配套的桌面应用的开发。
7.完成本文算法的调整、算法各模块定型、确定使用的参数。完成代码的集成，撰写毕业设计。
8.总结学习过程中遇到的问题，分析本文的缺点，并进行下一步的优化与改进的思考。（1）情感语料库的选择：
本系统将采用以下情感预料库：
EMO-DB[14]:该数据集是由10名演员(分别从5个男性和5个女性说话人的表演语音中获得)模拟7种情绪产生的10个德语语句,7种情绪分别是:中性、愤怒、恐惧、喜悦、悲伤、厌恶和厌倦,数据库共536个样本,该数据库已经成为许多研究的基础。
SAVEE[15]:该数据集是一个使用英国英语的多模态情感数据集。它总共包含了480条语音以及7种不同的情感:中性、快乐、悲伤、愤怒、惊讶、恐惧和厌恶。这些话语由4个专业的男性演员产生。为了保持情感表演的良好质量,本数据集的所有录音均由10位不同的评价者在音频、视觉和视听条件下进行验证这些录音中的脚本选自常规TIMIT语料库。
RAVDESS[16]:该数据集是情感语音和歌曲的多模态语料。该数据集是性别均衡的,由24名专门演员组成,他们以中性的北美（英语）发音产生语音和歌曲样本。对于情感性言语,它由平静、欢乐、悲伤、愤怒、恐惧、惊讶、厌恶构成。对于情感性的歌曲,它由平静、欢乐、悲伤、愤怒、恐惧、惊讶、厌恶和恐惧组成。每个表情都是在情感强度的两个层次上产生的,带有一个附加的中性表情。最后收集的7356份录音在情感有效性、强度和真实性方面分别被评为10次。对于这些收视率,雇佣了来自北美的247名未经培训的研究对象。
拟选用的上述数据库既可以完成多模态的同语言不同库的跨库识别(SAVEE和RAVDESA)实验,也可以完成跨语言跨库的识别实验（EMO-DB和SAVEE或EMO-DB和RAVDESS)。其中RAVDESS虽然是单模态的语料库库，但是由于其广泛的被采用，可以用来检验SER的基本识别性能，以及和已有的实验进行对比。
（2）语音特征的提取：
共振峰和MFCC（梅尔频率倒谱系数）是语音情感识别中两种常用的特征,共振峰（音质特征）是音质的决定因素,可以反映声道的物理特征;MFCC可以在很大程度上模拟人的听觉感知系统,从而提高语音情感识别的性能。可以采用传统的特征提取器open-SMILE，提取包括共振峰、MFCC等浅层特征。
基于深度学习的特征提取主要采用卷积神经网络（ConvolutionalNeuralNetworks,CNN）、循环神经网络(RecurrentNeuralNetwork,RNN)、深度神经网络（DeepNeuralNetwork,DNN）等方法,本系统拟采用经过改良的VGFCC谱特征。
伽马通滤波器组能有效模拟人耳的频谱分析与频率选择特性，基于Gammatone滤波器组提取的GFCC（gammatone）特征可以有效识别语音情感。语音信号是随时间改变的信号，传统的GFCC提取将信号直接经过快速傅里叶变换，设定信号在短时间内是近似不变的，反映的是语音信号的静态特性，忽略了信号本身的非线性、非稳态特性，不能全面地获取语音中的情感信息。通过自适应时频分析方法可以将一个复杂的信号分解为简单信号的线性组合，从而更深入地了解信号的本质属性，本系统拟将VMD（变分模态分解）算法应用于GFCC的提取过程，得到VGFCC特征。再通过特征级融合（韵律特征、非线性特征、VGFCC特征）得到包含更全面信息的全局特征GF。
（3）分类模型的构建：
本系统拟采用栈式堆叠稀疏自编码器与核函数极限学习机结合的复合网络模型（SSAE-KELM）复合网络作为识别模型。自编码器(AutomaticEncoders,AE)经过训练实现输入与输出的恒等，是一种无监督的学习算法，包括输入层、隐藏层、输出层三部分。AE通过向其本身强加一些约束，近似的复制并只能复制与训练数据相似的输入，这些约束强制模型考虑输入数据的哪些部分需要被优先复制，因此它往往能学习到数据的有用特性。
为了比较上述分类模型的有效性和可行性，本系统拟适当地采用传统的分类算法（比如HMM,GMM）进行额外实验。
（4）语音情感识别系统的开发：
本系统将在windows上开发，采用python语言和PySideQT技术进行图形界面的开发。深度学习框架使用pytorch。系统主要功能点如下：
用户注册登录。
算法和识别模型组合可选：
1.用户选择语料库。
2.用户选择要提取情感特征。
3.用户选择要使用的识别模型。
4.用户选定参数后进行训练，系统能够给出可视化训练过程。
语音文件的波形和谱图分析查看。
将抑郁症相关精神性疾病患者的语音文件导入进行分析，给出饼图，柱状图等形式的可视化分析的结果。
提供识别历史查询和管理。
